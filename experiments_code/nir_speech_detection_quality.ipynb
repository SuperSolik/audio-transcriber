{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeipetrov/Library/Caches/pypoetry/virtualenvs/test-xYtUzyB7-py3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sergeipetrov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "import pandas\n",
    "\n",
    "from math import sqrt, pow, exp\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeipetrov/Library/Caches/pypoetry/virtualenvs/test-xYtUzyB7-py3.10/lib/python3.10/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'ru_core_news_lg' (3.4.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "spacy_ru_stopwords = spacy.lang.ru.STOP_WORDS\n",
    "\n",
    "for word in spacy_ru_stopwords:\n",
    "    nlp.vocab[word].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_txt(path: str, segments_key=None) -> None:\n",
    "    path_ = Path(os.path.abspath(path))\n",
    "    name = path_.stem\n",
    "    file_dir = path_.parent\n",
    "\n",
    "    with open(str(path_), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if segments_key in data:\n",
    "        segments = data[segments_key]\n",
    "    else:\n",
    "        segments = data\n",
    "\n",
    "    content = ' '.join([r['text'] for r in segments])\n",
    "    with open(file_dir / f'{name}.txt', 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def convert_jsons_in_folder(folder: str):\n",
    "    for file_path in glob.glob(folder + '**/*.json', recursive=True):\n",
    "        json_to_txt(file_path, segments_key='segments')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_jsons_in_folder(\"./experiments/test1_noised\")\n",
    "convert_jsons_in_folder(\"./experiments/test2_noised\")\n",
    "# convert_jsons_in_folder(\"./experiments/test2/\")\n",
    "# convert_jsons_in_folder(\"./experiments/test3/\")\n",
    "# convert_jsons_in_folder(\"./experiments/test4/\")\n",
    "# convert_jsons_in_folder(\"./experiments/test5_full_day1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    text = text.replace('?', '').replace('!', '').replace('.', '').replace(',', '')\n",
    "    return text\n",
    "\n",
    "def process_text(text: str) -> list[str]:\n",
    "    tokens = nlp(remove_punctuation(text))\n",
    "    lemmas = [token.lemma_ for token in tokens]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def spacy_sim(example: str, to_validate: str) -> float:\n",
    "    doc1 = nlp(remove_punctuation(example))\n",
    "    doc2 = nlp(remove_punctuation(to_validate))\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "\n",
    "def jaccard_similarity(example: str, to_validate: str, is_raw: bool = True) -> float:\n",
    "    if is_raw:\n",
    "        x = list(filter(None, example.split()))\n",
    "        y = list(filter(None, to_validate.split()))\n",
    "    else:\n",
    "        x = process_text(example)\n",
    "        y = process_text(to_validate)\n",
    "\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "\n",
    "def squared_sum(x) -> float:\n",
    "    \"\"\" return 3 rounded square rooted value \"\"\"\n",
    "    return round(sqrt(sum([a*a for a in x])), 3)\n",
    "\n",
    "\n",
    "def euclidean_sim(example: str, to_validate: str) -> float:\n",
    "    def euclidean_distance(x,y):\n",
    "        \"\"\" return euclidean distance between two lists \"\"\"\n",
    "        return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
    "\n",
    "    x = nlp(remove_punctuation(example)).vector\n",
    "    y = nlp(remove_punctuation(to_validate)).vector\n",
    "\n",
    "    distance = euclidean_distance(x, y)\n",
    "    return 1 / exp(distance)\n",
    "\n",
    "\n",
    "def cosine_sim(example: str, to_validate: str) -> float:\n",
    "    \"\"\" return cosine similarity between two lists \"\"\"\n",
    "    x = nlp(remove_punctuation(example)).vector\n",
    "    y = nlp(remove_punctuation(to_validate)).vector\n",
    "    numerator = sum(a*b for a,b in zip(x, y))\n",
    "    denominator = squared_sum(x) * squared_sum(y)\n",
    "    return numerator / float(denominator)\n",
    "\n",
    "\n",
    "def find_sim(example_path: str, to_validate_path: str) -> list:\n",
    "    with open(example_path, 'r') as f:\n",
    "        example_text = f.read()\n",
    "    \n",
    "    path_ = Path(os.path.abspath(to_validate_path))\n",
    "    name = path_.stem\n",
    "\n",
    "    with open(to_validate_path, 'r') as f:\n",
    "        to_validate_text = f.read()\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"spacy\": spacy_sim(example=example_text, to_validate=to_validate_text),\n",
    "        \"jaccard_raw\": jaccard_similarity(example=example_text, to_validate=to_validate_text, is_raw=True),\n",
    "        \"jaccard_process\": jaccard_similarity(example=example_text, to_validate=to_validate_text, is_raw=False),\n",
    "        \"euclidean\": euclidean_sim(example=example_text, to_validate=to_validate_text),\n",
    "        \"cosine\": cosine_sim(example=example_text, to_validate=to_validate_text),\n",
    "    }\n",
    "\n",
    "def find_sims(example_path: str, folder: str) -> list[dict]:\n",
    "    # print(f\"{folder=}\")\n",
    "    data = []\n",
    "    for file_path in glob.glob(folder + '**/*.txt', recursive=True):\n",
    "        data.append(\n",
    "            find_sim(example_path, file_path)\n",
    "        )\n",
    "    # pprint(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_compare = [\n",
    "    (\"./nir_audio/example_1.txt\", \"./experiments/test1_noised/\"),\n",
    "    (\"./nir_audio/example_2.txt\", \"./experiments/test2_noised/\"),\n",
    "]\n",
    "\n",
    "frames = []\n",
    "\n",
    "for example, folder in data_to_compare:\n",
    "    sim_data = find_sims(example_path=example, folder=folder)\n",
    "    data_for_pandas = defaultdict(list)\n",
    "    for d in sim_data:\n",
    "        for k, v in d.items():\n",
    "            data_for_pandas[k].append(v)\n",
    "\n",
    "    df = pandas.DataFrame(data_for_pandas)\n",
    "    frames.append(df)\n",
    "    df.to_csv(f\"{folder}/result.csv\", decimal=',')\n",
    "\n",
    "df_combined = pandas.concat(frames)\n",
    "df_combined.to_csv(\"./experiments/noised_combined_result.csv\", decimal=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 ('test-xYtUzyB7-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de6c00059cca1274e56f0642d260d061e936284734c52ac31c9696b16ca70d1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
